{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# Check if MPS can be used\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MPS device\n",
    "device = torch.device(\"mps\")\n",
    "# 항상 모델 코드를 짜고 난 후에 model = 내가정의한모델().to(device) 이렇게 추가해야 GPU 위에서 돌아가게 할 수 있다.\n",
    "# Pytorch는 input data와 model이 같은 device 위에 올려져야하기 때문에 inputs, labels = inputs.to(device), labels.to(device) 이렇게도 해줘야한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tensors\n",
    "\n",
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar is one of tensor\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim # Because scalar has no dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()\n",
    "# Convert tensor into Python int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([7,7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.ndim # Why 1 dim? Because we have 1 square bracket. []의 갯수가 차원의 수이다.\n",
    "# Because it's a 1D Array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape # Why shape 2? Because 2 elements along single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MATRIX\n",
    "MATRIX = torch.tensor([[7, 8],\n",
    "                       [9, 10]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape # 2x2 matrix이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.ndim # Should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TENSOR\n",
    "TENSOR = torch.tensor([[[1,2,3], \n",
    "                        [3,6,9],\n",
    "                        [2,4,5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [3, 6, 9],\n",
       "        [2, 4, 5]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(TENSOR[0][0])\n",
    "print(TENSOR[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tensors\n",
    "\n",
    "Why random tensors? Weight initialization is random.\n",
    "\n",
    "NN starts with tensors full of random numbers and then adjust those random numbers to better represent the data.\n",
    "\n",
    "\"Adjusting numbers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2356, 0.5711, 0.1637, 0.6672, 0.3183],\n",
       "         [0.2058, 0.7357, 0.3615, 0.7486, 0.0631],\n",
       "         [0.0323, 0.0301, 0.7154, 0.1409, 0.6231],\n",
       "         [0.1667, 0.8137, 0.9611, 0.2009, 0.8688],\n",
       "         [0.1874, 0.2085, 0.6529, 0.2228, 0.9313]],\n",
       "\n",
       "        [[0.4538, 0.7867, 0.3547, 0.9298, 0.2351],\n",
       "         [0.2772, 0.0715, 0.7376, 0.8142, 0.0876],\n",
       "         [0.9019, 0.0673, 0.4857, 0.1964, 0.6893],\n",
       "         [0.7400, 0.6828, 0.9329, 0.9987, 0.0818],\n",
       "         [0.4620, 0.5614, 0.3103, 0.8723, 0.3265]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor of size (3,4)\n",
    "random_tensor = torch.rand(2, 5, 5)  # 3 rows, 4 columns\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3281, 0.6812, 0.3186,  ..., 0.2321, 0.9316, 0.2629],\n",
       "         [0.5251, 0.4432, 0.6910,  ..., 0.4855, 0.6138, 0.6891],\n",
       "         [0.0400, 0.8871, 0.5627,  ..., 0.9201, 0.0858, 0.0136],\n",
       "         ...,\n",
       "         [0.3644, 0.7590, 0.2259,  ..., 0.6545, 0.0742, 0.9440],\n",
       "         [0.7935, 0.5869, 0.3889,  ..., 0.2436, 0.5092, 0.2302],\n",
       "         [0.5047, 0.9402, 0.3230,  ..., 0.7082, 0.4200, 0.6654]],\n",
       "\n",
       "        [[0.6802, 0.7121, 0.3442,  ..., 0.7751, 0.7457, 0.1589],\n",
       "         [0.1523, 0.8453, 0.6861,  ..., 0.3257, 0.7034, 0.6286],\n",
       "         [0.3011, 0.4610, 0.0237,  ..., 0.1967, 0.1305, 0.8388],\n",
       "         ...,\n",
       "         [0.1355, 0.0014, 0.4999,  ..., 0.7620, 0.1214, 0.7550],\n",
       "         [0.6072, 0.9784, 0.9093,  ..., 0.5892, 0.4425, 0.7693],\n",
       "         [0.9017, 0.2845, 0.8342,  ..., 0.2741, 0.5360, 0.0689]],\n",
       "\n",
       "        [[0.8357, 0.1172, 0.4153,  ..., 0.4261, 0.1444, 0.0726],\n",
       "         [0.6335, 0.3890, 0.8252,  ..., 0.4942, 0.3523, 0.7716],\n",
       "         [0.3683, 0.2749, 0.5755,  ..., 0.6344, 0.3500, 0.4942],\n",
       "         ...,\n",
       "         [0.2615, 0.8598, 0.4054,  ..., 0.4518, 0.2558, 0.5539],\n",
       "         [0.8701, 0.1149, 0.7114,  ..., 0.2549, 0.2753, 0.9807],\n",
       "         [0.1972, 0.2648, 0.2197,  ..., 0.9063, 0.7595, 0.9688]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor with similar shape to an image tensor\n",
    "random_image_size_tensor = torch.rand(3, 256, 256) #행, 열, RGB(레드 그린 블루 패널 3개)\n",
    "random_image_size_tensor\n",
    "# popular shape: color x height x width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeros and Ones\n",
    "\n",
    "Useful when masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(3,4)  # What size do I need to wipe out the first column?\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8642, 0.7834, 0.4105, 0.7751],\n",
       "        [0.2258, 0.6823, 0.6717, 0.1237],\n",
       "        [0.7000, 0.5160, 0.1975, 0.0268]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random1 = torch.rand(3,4)\n",
    "random1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tensor = zeros * random1\n",
    "masked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.dtype # Default data type의 줄임말이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a range of tensors and tensors-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_to_ten = torch.arange(1, 11)\n",
    "one_to_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as python range indexing\n",
    "skip_tensor = torch.arange(1, 100, 10)\n",
    "skip_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어떤 텐서가 있을 때, 그 텐서랑 같은 shape를 하도록 만들어 줄 수 있다.\n",
    "# Creating tensors like\n",
    "\n",
    "ten_zeros = torch.zeros_like(one_to_ten)\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor data types\n",
    "\n",
    "**Note: Tensor datatypes is one of the 3 big errors you'll run into Pytorch:\n",
    "1. Tensors not right datatype\n",
    "2. Tensors not the right shape\n",
    "3. Tensors not on the right device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.], device='mps:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0], \n",
    "                               dtype=None, \n",
    "                               device=device, \n",
    "                               requires_grad=False)\n",
    "\n",
    "float_32_tensor\n",
    "\n",
    "\n",
    "# These 3 are the most important parameters of a tensor.\n",
    "# dtype is for datatype, which goes like: float32(pytorch default), float64, float16\n",
    "# float16 sacrifices detail half than float32 but computes 2 times faster(less memory)\n",
    "# device = MPS, so we can use GPU as: device = device.\n",
    "# requires_grad: whether or not to track gradients with this tensors operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Tensor Attributes\n",
    "\n",
    "1. Shape : tensor.dtype\n",
    "2. datatype : tensor.shape\n",
    "3. device : tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([3, 4])\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "some_tensor = torch.rand(3,4)\n",
    "some_tensor = some_tensor.to(device)  # Move our tensor to device\n",
    "print(some_tensor.dtype)\n",
    "print(some_tensor.shape)\n",
    "print(some_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations\n",
    "\n",
    "Tensor operations include:\n",
    "\n",
    "1. Addition\n",
    "2. Subtraction\n",
    "3. Multiplication (element-wise) *If you want matrix multiplication, use matmul\n",
    "4. Division\n",
    "5. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor1 += 10\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([110, 120, 130])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplication\n",
    "tensor1 *= 10\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1100, 1200, 1300])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can try bulit-in function\n",
    "torch.mul(tensor1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Division\n",
    "tensor1 //= 10\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplcation\n",
    "\n",
    "1. Element-wise multiplication (그냥 각 항목을 상수배 해주는)\n",
    "2. Dot Product (matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(434)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor1, tensor1) # Dot product result is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1.4824,  17.5870],\n",
       "        [ 14.4894,   5.6143],\n",
       "        [  4.6278,  -4.5474],\n",
       "        [-24.6648,   6.8049]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2 = torch.randn(4, 100)\n",
    "tensor3 = torch.randn(100,2)\n",
    "result1 = torch.matmul(tensor2, tensor3)\n",
    "result1\n",
    "\n",
    "# The results would be 4 x 2. The 100 is like the weights of the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x100 and 3x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Vector transpose\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tensor4 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor4\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x100 and 3x100)"
     ]
    }
   ],
   "source": [
    "# Vector transpose through .T\n",
    "# The code below will produce an error\n",
    "tensor4 = torch.randn(3, 100)\n",
    "torch.matmul(tensor2, tensor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-24.2405,  -3.0962,   2.6595],\n",
       "        [ -8.4644,  -9.2487,   1.6615],\n",
       "        [ 13.8404,   9.4026,  -4.3244],\n",
       "        [  3.5046,  -4.3973,  -6.0458]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor2, tensor4.T) # If we transpose it and make the shape 100 x 3, matmul!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Aggregation\n",
    "\n",
    "### Finding the min, max, mean, sum, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90]), torch.int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(0, 100, 10)\n",
    "x, x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90)\n",
      "<built-in method max of Tensor object at 0x14de830e0>\n",
      "tensor(0)\n",
      "<built-in method min of Tensor object at 0x14de830e0>\n",
      "tensor(45.)\n",
      "<built-in method mean of Tensor object at 0x14de830e0>\n",
      "tensor(450)\n",
      "<built-in method sum of Tensor object at 0x14de830e0>\n"
     ]
    }
   ],
   "source": [
    "# Note: torch.mean() requires a tensor of float32 datatype to work\n",
    "\n",
    "print(torch.max(x))\n",
    "print(x.max)\n",
    "print(torch.min(x))\n",
    "print(x.min)\n",
    "print(torch.mean(x.type(torch.float32))) \n",
    "print(x.mean)\n",
    "print(torch.sum(x))\n",
    "print(x.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the positional min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the position in tensor that has the minimun value\n",
    "# Returns index position of taret tensor where the minimum value occurs\n",
    "x.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping, stacking, squeezing, and unsqueezing tensors\n",
    "\n",
    "* Reshaping: Reshapes an input tensor to a defined shape\n",
    "\n",
    "* Stacking: Combine multiple tensors on top / side by side\n",
    "\n",
    "* Squeeze: Removes all '1' dimensions from a tensor\n",
    "\n",
    "* Unsqueeze: Add a '1' dimension to a target tensor\n",
    "\n",
    "* Permute: Return a view of the input with dimensions permuted (swapped) in a certain way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), torch.Size([10]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor5 = torch.arange(1., 11.)\n",
    "tensor5, tensor5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.],\n",
       "         [ 3.,  4.],\n",
       "         [ 5.,  6.],\n",
       "         [ 7.,  8.],\n",
       "         [ 9., 10.]]),\n",
       " torch.Size([5, 2]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping tensors. \n",
    "# The reshaped result \"must conserve the number of elements\"\n",
    "# In this case, we only have 10 (torch.Size[10])\n",
    "\n",
    "tensor_reshaped = tensor5.reshape(5,2) # 5 x 2 is 10\n",
    "tensor_reshaped, tensor_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "        [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking tensors\n",
    "\n",
    "stacked_tensor = torch.stack([tensor5, tensor5, tensor5, tensor5])\n",
    "stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.],\n",
       "         [ 3.,  4.],\n",
       "         [ 5.,  6.],\n",
       "         [ 7.,  8.],\n",
       "         [ 9., 10.]]),\n",
       " torch.Size([5, 2]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squeezing tensors\n",
    "# Note: Squeezing removes \"1\" dimension\n",
    "\n",
    "squeezed_tensor = tensor_reshaped.squeeze()\n",
    "squeezed_tensor, squeezed_tensor.shape\n",
    "\n",
    "# This does not remove anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9]),\n",
       " tensor([0.7492, 0.2213, 0.7740, 0.1490, 0.3385, 0.8517, 0.6635, 0.2477, 0.8125]),\n",
       " torch.Size([9]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But this does!\n",
    "\n",
    "tensor6 = torch.rand(1,9)\n",
    "squeezed_tensor2 = tensor6.squeeze()\n",
    "tensor6.shape, squeezed_tensor2, squeezed_tensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7492],\n",
       "         [0.2213],\n",
       "         [0.7740],\n",
       "         [0.1490],\n",
       "         [0.3385],\n",
       "         [0.8517],\n",
       "         [0.6635],\n",
       "         [0.2477],\n",
       "         [0.8125]]),\n",
       " torch.Size([9, 1]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unsqueeze, adds a 1 dimension at the position of parameter \"dim=\"\n",
    "unsqueezed = squeezed_tensor2.unsqueeze(dim=1)\n",
    "unsqueezed, unsqueezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Permute\n",
    "# Same as permutation matrix (swapping rows)\n",
    "\n",
    "tensor7 = torch.rand(256, 256, 3)\n",
    "permuted = tensor7.permute(2, 1, 0) #2번째 차원(3)이 맨 앞으로, 1번째 차원(256)이 그 다음, 0번째 차원이 다음)\n",
    "permuted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor8 = torch.tensor([[[1,2,3],\n",
    "                       [4,5,6],\n",
    "                       [7,8,9]]])\n",
    "tensor8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to ouput 9?\n",
    "# : stands for \"select all values of that dimension\"\n",
    "\n",
    "tensor8[0][2][2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 6, 9]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to output 3, 6, 9?\n",
    "tensor8[:, :, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "#### The expressions `tensor[:][:][2]` and `tensor[:,:,2]` differ because of how indexing works in multi-dimensional tensors (or arrays).\n",
    "\n",
    "### 1. `tensor[:][:][2]`\n",
    "- **Step-by-step breakdown**:\n",
    "  - `tensor[:]` creates a view or shallow copy of the tensor.\n",
    "  - `tensor[:][:]` applies another slice operation to the result of `tensor[:]`, which is effectively the same as the first slice. However, it's less clear and could potentially change the structure depending on the implementation.\n",
    "  - `tensor[:][:][2]` then takes the third element (index 2) of this sliced structure, which might not be the intended operation on the original tensor.\n",
    "\n",
    "### 2. `tensor[:,:,2]`\n",
    "- **Explanation**:\n",
    "  - This syntax directly indexes the third dimension (or channel) across all rows and columns of the tensor.\n",
    "  - `:` indicates selecting all elements along that dimension, while `2` selects the third element of the last dimension.\n",
    "\n",
    "### Why They Are Different\n",
    "- `tensor[:][:][2]` does not directly select the third \"slice\" in the third dimension of a tensor. Instead, it first modifies the tensor’s shape with each slicing, eventually resulting in a 1D array (or a vector). Then it selects the third element of that 1D array.\n",
    "- `tensor[:,:,2]`, however, is a direct slicing of the third dimension and is the correct way to access that particular slice in a multi-dimensional tensor.\n",
    "\n",
    "In short, `tensor[:,:,2]` is the correct and direct way to slice the third element along the third dimension, while `tensor[:][:][2]` ends up being a chain of operations that can yield unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors & Numpy\n",
    "\n",
    "* Data in Numpy -> Pytorch tensor : torch.from_numpy(ndarray)\n",
    "* Pytorch tensor -> NumPy -> torch.Tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor9 = torch.from_numpy(array)\n",
    "array, tensor9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When converting numpy -> tensor, default numpy dtype is floa64, while pytorch is float32\n",
    "tensor10 = tensor9.type(torch.float32)\n",
    "tensor10.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility of tensors (Random seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# Controled randomness. It works only one block of code at a time.\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "rand_tensor1 = torch.rand(3,4)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "rand_tensor2 = torch.rand(3,4)\n",
    "\n",
    "print(rand_tensor1 == rand_tensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puting tensors (and models) on the GPU\n",
    "\n",
    "For faster computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    }
   ],
   "source": [
    "tensor11 = torch.tensor([1,2,3])\n",
    "print(tensor11, tensor11.device)\n",
    "# Currently on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='mps:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's move this tensor to GPU\n",
    "tensor11_gpu = tensor11.to(device)\n",
    "tensor11_gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
